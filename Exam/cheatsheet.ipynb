{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_FOLDER = '.\\\\data\\\\'\n",
    "DATASET = DATA_FOLDER+\"dataset.csv\"\n",
    "\n",
    "# example read\n",
    "df = pd.read_csv(DATASET, compression='infer', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "        Colored comment\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming Spark session is created (spark), and Spark context is created (sc)\n",
    "\n",
    "# Read JSON file and create a DataFrame\n",
    "df = spark.read.json(\"dataset.json.gz\")\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squaring each element in the RDD\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Filtering even numbers from the RDD\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Flattening each element into a range of three numbers\n",
    "words_rdd = rdd.flatMap(lambda x: list(range(x, x + 3)))\n",
    "\n",
    "# Sampling the RDD without replacement, keeping 50% of the data\n",
    "sampled_rdd = rdd.sample(withReplacement=False, fraction=0.5)\n",
    "\n",
    "# Union of two RDDs\n",
    "union_rdd = rdd1.union(rdd2)\n",
    "\n",
    "# Intersection of two RDDs\n",
    "intersect_rdd = rdd1.intersection(rdd2)\n",
    "\n",
    "# Distinct elements in the RDD\n",
    "distinct_rdd = rdd.distinct()\n",
    "\n",
    "### PAIRS ###\n",
    "\n",
    "# Creating a Pair RDD and grouping values by key\n",
    "pair_rdd = sc.parallelize([(1, 'a'), (2, 'b'), (1, 'c')])\n",
    "grouped_rdd = pair_rdd.groupByKey()\n",
    "\n",
    "# Reducing by key (summing values with the same key)\n",
    "sum_rdd = pair_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sorting the Pair RDD by key\n",
    "sorted_rdd = pair_rdd.sortByKey()\n",
    "\n",
    "# Creating another Pair RDD and performing an inner join\n",
    "rdd3 = sc.parallelize([(1, 'apple'), (2, 'banana'), (3, 'orange')])\n",
    "joined_rdd = pair_rdd.join(rdd3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all elements of the squared_rdd \n",
    "collected_data = squared_rdd.collect()\n",
    "\n",
    "# Count the total number of elements\n",
    "count = rdd.count()\n",
    "\n",
    "# Take the first three elements from the rdd\n",
    "first_three = rdd.take(3)\n",
    "\n",
    "# Save the elements of squared_rdd as a text file\n",
    "squared_rdd.saveAsTextFile(\"output/squared_numbers\")\n",
    "\n",
    "# Count the occurrences of each key in the pair_rdd\n",
    "key_counts = pair_rdd.countByKey()\n",
    "\n",
    "# Compute statistics about the elements in the rdd, such as mean, variance, etc.\n",
    "rdd_stats = rdd.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links: 15791\n",
      "Total number of items: 464\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# example URL\n",
    "URL = 'http://dblp.uni-trier.de/pers/hd/v/Vetterli:Martin'\n",
    "\n",
    "# get the parsed html from URL\n",
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# get the parsed elements with find_all\n",
    "all_links = soup.find_all('a')\n",
    "print('Total number of links: {0}'.format(len(all_links)))\n",
    "\n",
    "all_items = soup.find_all('li', class_='entry')\n",
    "print('Total number of items: {0}'.format(len(all_items)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'already', 'far', 'north', 'of', 'London', ',', 'and', 'as', 'I', 'walk', 'in', 'the', 'streets', 'of', 'Petersburgh', ',', 'I', 'feel', 'a', 'cold', 'northern', 'breeze', 'play', 'upon', 'my', 'cheeks', ',', 'which', 'braces', 'my', 'nerves', 'and', 'fills', 'me', 'with', 'delight', '.']\n",
      "[far, north, London, ,, walk, streets, Petersburgh, ,, feel, cold, northern, breeze, play, cheeks, ,, braces, nerves, fills, delight, .]\n",
      "['far', 'north', 'London', ',', 'walk', 'street', 'Petersburgh', ',', 'feel', 'cold', 'northern', 'breeze', 'play', 'cheek', ',', 'brace', 'nerve', 'fill', 'delight', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example sentence\n",
    "example = 'I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight.'\n",
    "\n",
    "# Tokenization\n",
    "tokens = nlp(example)\n",
    "\n",
    "# tokenize\n",
    "tokenized = [token.text for token in tokens]\n",
    "print(tokenized)\n",
    "\n",
    "# remove stop words\n",
    "stop_word_removed = [token for token in tokens if not token.is_stop]\n",
    "print(stop_word_removed)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatized = [token.lemma_ for token in tokens if not token.is_stop]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Geometric mean\n",
    "- used when the data is heavy tailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.057457376233529\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "def geometric_mean(col: Union[List, pd.Series]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the geometric mean of a list or pandas Series containing numerical values.\n",
    "\n",
    "    Parameters:\n",
    "    - col (Union[List, pd.Series]): Input data, a list or pandas Series of numerical values.\n",
    "\n",
    "    Returns:\n",
    "    - float: Geometric mean of the input data.\n",
    "    \"\"\"\n",
    "    return np.exp(np.log(col).mean())\n",
    "\n",
    "# Example usage:\n",
    "data = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "print(geometric_mean(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlations\n",
    "- Pearson\n",
    "    - works best in linear correlations\n",
    "- Spearman\n",
    "    - robust to outliers\n",
    "    - relies on the rank of datapoints (rather than on actual values)\n",
    "    - works best with [monotonic](https://en.wikipedia.org/wiki/Monotonic_function) functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats                     # statistical correlation - .spearmanr, .pearsonr   \n",
    "                                            # comparison - .ttest_ind\n",
    "\n",
    "from statsmodels.stats import diagnostic    # check data disctribution - .kstest_normal / .kstest_exponential\n",
    "\n",
    "import statsmodels.formula.api as smf       # ordinary least square - smf.ols(formula, data).fit().summary()\n",
    "                                            # logistic least square - smf.logit(formula, data).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bootstrap CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7.6, 14.8)\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_confidence_interval(array, iterations = 1000, ci_level = 95):\n",
    "    \"\"\"\n",
    "    Bootstrap the 95% confidence interval for the mean of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: An array of data\n",
    "    - iterations: The number of bootstrap samples to generate\n",
    "    - ci_level: CI percentage\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple representing the lower and upper bounds of the 95% confidence interval\n",
    "    \"\"\"\n",
    "    means = np.zeros(iterations)\n",
    "    alpha = 100 - ci_level\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        bootstrap_sample = np.random.choice(a=array, size=len(array), replace=True)\n",
    "        means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "    lower = np.percentile(means, alpha / 2)\n",
    "    upper = np.percentile(means, 100 - alpha / 2)\n",
    "    \n",
    "    return lower, upper\n",
    "# Example usage:\n",
    "data = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "print(bootstrap_confidence_interval(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cool groupby\n",
    "```python\n",
    "stats_by_year = movies.groupby(movies['year']).apply(lambda x: pd.Series({\n",
    "        'average_worldwide_gross': x['worldwide_gross'].mean(),\n",
    "        'std_dev_worldwide_gross': x['worldwide_gross'].std()\n",
    "    }))\n",
    "```\n",
    "same as\n",
    "```python\n",
    "movies.groupby('year')['worldwide_gross'].agg(['mean','std']).rename({\n",
    "        'mean':'average_worldwide_gross',\n",
    "        'std':'std_dev_worldwide_gross'\n",
    "    },axis=1)\n",
    "```\n",
    "NOTE: ['worldwide_gross'] -- NOT [['worldwide_gross']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regression analysis - __04-Regression analysis__\n",
    "\n",
    "**Interation term e.g.**\\\n",
    "Suppose you are modeling the relationship between income (X1​), education (X2​), and job satisfaction (Y). The interaction term X1​×X2​ would capture whether the effect of income on job satisfaction depends on the level of education.\\\n",
    "If it​ is significant, it suggests that the effect of income (X1​) on the job satisfaction (Y) depends on the level of the education (X2​).\n",
    "\n",
    "**R squared**\\\n",
    "R-Squared determines the proportion of variance in the dependent variable (Y) that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit). R-squared can take any values between 0 to 1.\\\n",
    "For example, an r-squared of 60% reveals that 60% of the variability observed in the target variable is explained by the regression model. Generally, a higher r-squared indicates more variability is explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Propensity score matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def propensity_score_matching(df: pd.DataFrame,\n",
    "                 treat_column: str,\n",
    "                 continuous_features: List[str] = [],\n",
    "                 categorical_features: List[str] = []) -> List[int]:\n",
    "    \"\"\"\n",
    "    Balances a dataset based on propensity scores for treatment and control groups.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the original data.\n",
    "    - treat_column (str): The column indicating treatment assignment (1 for treatment, 0 for control).\n",
    "    - continuous_features (List[str]): List of names of continuous features to be standardized.\n",
    "    - categorical_features (List[str]): List of names of categorical features for logistic regression.\n",
    "\n",
    "    Returns:\n",
    "    - List[int]: List of indices of matched instances in the original DataFrame.\n",
    "    \"\"\"\n",
    "    assert (len(continuous_features) != 0) | (len(categorical_features) != 0), 'no feature passed to be matched on'\n",
    "    # Copy the df to avoid modifying the original dataframe\n",
    "    data = df[[treat_column] + continuous_features + categorical_features]\n",
    "    \n",
    "    # Standardize the continuous features\n",
    "    for column in continuous_features:\n",
    "        data[column] = (data[column] - data[column].mean())/data[column].std()\n",
    "\n",
    "    # Get the formula\n",
    "    formula = ' + '.join(continuous_features + categorical_features)    \n",
    "    formula = f\"{treat_column} ~ {formula}\"\n",
    "\n",
    "    # Estimate propensity scores\n",
    "    mod = smf.logit(formula=formula, data=data)\n",
    "    res = mod.fit()\n",
    "    data['Propensity_score'] = res.predict()\n",
    "\n",
    "    # Calculate similarity\n",
    "    def get_similarity(propensity_score1, propensity_score2):\n",
    "        '''Calculate similarity for instances with given propensity scores'''\n",
    "        return 1-np.abs(propensity_score1-propensity_score2)\n",
    "\n",
    "    # Split into control/treat groups \n",
    "    treatment_df = data[data[treat_column] == 1]\n",
    "    control_df = data[data[treat_column] == 0]\n",
    "\n",
    "    # Balance the dataset \n",
    "    G = nx.Graph()\n",
    "    for control_id, control_row in control_df.iterrows():\n",
    "        for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "            similarity = get_similarity(control_row['Propensity_score'], treatment_row['Propensity_score'])\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "    matching = nx.max_weight_matching(G)\n",
    "    matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "    \n",
    "    return matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine Learning\n",
    "**Confusion Matrix**\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross validation best score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "\n",
    "## CV regression\n",
    "reg = Ridge()\n",
    "param_grid = {'alpha':[1e-1, 1e-2, 1e-3]}\n",
    "cv_model = GridSearchCV(reg, param_grid=param_grid, cv=3)\n",
    "\n",
    "## CV classification\n",
    "# clf = LogisticRegression(random_state=42)\n",
    "# param_grid = {'C':[1,10,100]}\n",
    "# cv_model = GridSearchCV(clf, param_grid=param_grid, cv=3)\n",
    "\n",
    "cv_model = cv_model.fit(X_train, y_train)\n",
    "print(cv_model.best_score_)\n",
    "print(\"The optimal alpha is: %r\" % (cv_model.best_params_['alpha']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_model(X,y,C):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    model = LogisticRegression(max_iter=2000, C=C).fit(X_train, y_train)\n",
    "    y_predicted = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "\n",
    "    print('Test Accuracy:',round(accuracy,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def evaluate(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    Calculate classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: True labels.\n",
    "    - y_predicted: Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing various classification metrics.\n",
    "    \"\"\"\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_predicted)\n",
    "    tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "    precision = metrics.precision_score(y_true, y_predicted)\n",
    "    recall = metrics.recall_score(y_true, y_predicted)\n",
    "    accuracy = metrics.accuracy_score(y_true, y_predicted)\n",
    "    f1_score = metrics.f1_score(y_true, y_predicted)\n",
    "\n",
    "    return {\n",
    "        'True Positive': tp,\n",
    "        'False Positive': fp,\n",
    "        'True Negative': tn,\n",
    "        'False Negative': fn,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering imports**\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN (see lab 8 for DBSCAN)\n",
    "```\n",
    "\n",
    "**PCA**\n",
    "```python\n",
    "X_reduced_pca = PCA(n_components=2).fit_transform(X)\n",
    "```\n",
    "\n",
    "**TSNE**\n",
    "```python\n",
    "X_reduced_tsne = TSNE(n_components=2, init='random', learning_rate='auto', random_state=0).fit_transform(X)\n",
    "```\n",
    "\n",
    "**Normalize**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "pd.DataFrame(X_norm)\n",
    "```\n",
    "\n",
    "**Example of Clustering Pipeline**\n",
    "```python\n",
    "# rescale\n",
    "X_wine = StandardScaler().fit_transform(wine)\n",
    "\n",
    "# best k\n",
    "silhouette = []\n",
    "for k in range(2,11):\n",
    "    labels = KMeans(n_clusters=k,random_state=42).fit_predict(X_wine)\n",
    "    score = silhouette_score(X_wine,labels)\n",
    "    silhouette.append([k,score])\n",
    "\n",
    "argmax = np.array(silhouette)[:,1].argmax()\n",
    "k_best = silhouette[argmax][0]\n",
    "\n",
    "# pca\n",
    "pca_wine = PCA(n_components=2).fit_transform(X_wine)\n",
    "\n",
    "# kmeans\n",
    "labels_wine = KMeans(n_clusters=k_best,random_state=42).fit_predict(X_wine)\n",
    "\n",
    "# plot\n",
    "plt.scatter(pca_wine[:,0],pca_wine[:,1],c=labels_wine)\n",
    "plt.title('PCA')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n-grams with frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('apple', 'orange'), 2), (('banana', 'grape'), 2), (('grape', 'kiwi'), 2), (('kiwi', 'apple'), 2), (('orange', 'banana'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def generate_ngrams_with_frequency(words, n, n_common: int = None):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of words using nltk and calculate their frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    - words (list): List of words.\n",
    "    - n (int): Size of the n-grams.\n",
    "    - n_common: Number of most common n-grams\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples containing n-grams and their frequencies.\n",
    "    \"\"\"\n",
    "    if n > 1:\n",
    "        words = list(ngrams(words, n))\n",
    "    n_gram_counts = Counter(words)\n",
    "    if n_common is not None: \n",
    "        n_gram_counts = n_gram_counts.most_common(n_common)\n",
    "        return [(n_gram, count) for n_gram, count in n_gram_counts]\n",
    "    else:\n",
    "        return [(n_gram, count) for n_gram, count in n_gram_counts.items()]\n",
    "\n",
    "# Example usage:\n",
    "word_list = [\"apple\", \"orange\", \"banana\", \"grape\", \"kiwi\", \"apple\", \"orange\", \"kiwi\", \"apple\", \"banana\", \"grape\", \"kiwi\", \"orange\"]\n",
    "n_value = 2\n",
    "result = generate_ngrams_with_frequency(word_list, n_value, 5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'sentence', 'contains', 'words', 'want', 'clean', 'preprocess', 'analysis', 'cleaning', 'includes', 'converting', 'lowercase', 'removing', 'non', 'english', 'letter', 'characters', 'tokenization', 'lemmatization', 'removing', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text_data: str, remove_stopwords: bool = True, lemmatize: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data.\n",
    "\n",
    "    Parameters:\n",
    "    - text_data (str): Input text data.\n",
    "    - remove_stopwords (bool): Flag to indicate whether to remove stopwords or not.\n",
    "    - lemmatize (bool): Flag to indicate whether to lemmatize words or not.\n",
    "\n",
    "    Returns:\n",
    "    - text_data_clean (list): List of cleaned and preprocessed words.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text_data = text_data.lower()\n",
    "\n",
    "    # Remove all non-English-letter characters\n",
    "    text_data = re.sub(r'[^a-z]', ' ', text_data)\n",
    "\n",
    "    # Create a list of words\n",
    "    text_data = nltk.word_tokenize(text_data)\n",
    "\n",
    "    # Lemmatize the words if lemmatize is True\n",
    "    if lemmatize:\n",
    "        wl = WordNetLemmatizer()\n",
    "        text_data = [wl.lemmatize(word) for word in text_data]\n",
    "\n",
    "    # Remove stopwords if remove_stopwords is True\n",
    "    if remove_stopwords:\n",
    "        text_data = [word for word in text_data if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    # Remove single letters\n",
    "    text_data_clean = [word for word in text_data if len(word) > 1]\n",
    "\n",
    "    return text_data_clean\n",
    "\n",
    "# Example usage:\n",
    "text_data = \"This is an example sentence. It contains some words that we want to clean and preprocess for further analysis. Cleaning includes converting to lowercase, removing non-English-letter characters, tokenization, lemmatization, and removing stopwords.\"\n",
    "cleaned_words = clean_text(text_data, remove_stopwords=True, lemmatize=False)\n",
    "print(cleaned_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment analysis\n",
    "[link](https://medium.com/@rslavanyageetha/vader-a-comprehensive-guide-to-sentiment-analysis-in-python-c4f1868b0d2e#:~:text=text%20%3D%20%22I%20love%20Python!%22%0A%0Ascores%20%3D%20analyzer.polarity_scores(text)%0A%0Aprint(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BOW and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46941728 0.61722732 0.3645444  0.         0.\n",
      "  0.3645444  0.         0.3645444 ]\n",
      " [0.         0.7284449  0.         0.28285122 0.         0.47890875\n",
      "  0.28285122 0.         0.28285122]\n",
      " [0.49711994 0.         0.         0.29360705 0.49711994 0.\n",
      "  0.29360705 0.49711994 0.29360705]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "def vectorize(doc: np.array, vectorizer_type: str = 'bow'):\n",
    "    \"\"\"\n",
    "    Vectorize documents using either Bag-of-Words (bow) or TF-IDF vectorizer.\n",
    "\n",
    "    Parameters:\n",
    "    - doc (np.array): Array of text documents.\n",
    "    - vectorizer_type (str): Type of vectorizer to use ('bow' or 'tfidf').\n",
    "\n",
    "    Returns:\n",
    "    - Vectorized representation of the documents.\n",
    "    \"\"\"\n",
    "    if vectorizer_type == 'bow':\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif vectorizer_type == 'tfidf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid vectorizer type. Use 'bow' or 'tfidf'.\")\n",
    "\n",
    "    return vectorizer.fit_transform(doc).toarray()\n",
    "\n",
    "# Example usage:\n",
    "documents = [\n",
    "    \"This is the first document.\", \n",
    "    \"This document is the second document.\", \n",
    "    \"And this is the third one.\"\n",
    "    ]\n",
    "vectorized_bow = vectorize(documents, vectorizer_type='bow')\n",
    "vectorized_tfidf = vectorize(documents, vectorizer_type='tfidf')\n",
    "\n",
    "print(vectorized_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sparsity\n",
    "\"Sparsity\" of a graph with $n$ nodes is defined as follows: \n",
    "\n",
    "$ L = \\frac{|E|}{|E_{max}|}$, where $E_{max} = \\frac{n * (n-1)}{2}$\n",
    "\n",
    "$ L = \\text{number of edges} / \\text{potential number of edges} $\n",
    "```python\n",
    "nx.density(G)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connected componets\n",
    "```python\n",
    "list(nx.connected_components(G)) # list of dictionaries\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diameter\n",
    "\"Diameter\" = longest shortest-path\n",
    "\n",
    "```python\n",
    "nx.diameter(G)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Triadic Closure\n",
    "\n",
    "__Clustering coefficient__ of a node\n",
    "```python\n",
    "nx.clustering(G, ['node'])\n",
    "```\n",
    "__Global Clustering coefficient__, or the ratio of all existing triangles over all possible triangles.\n",
    "```python\n",
    "nx.transitivity(G)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centralities\n",
    "\n",
    "- Degree centrality of node i = number of neighbours of node i\n",
    "- Closeness centrality of node i = sum(1 / sum of distances from node i to other nodes)\n",
    "- Betweenness centrality of node i = fraction of shortest pathes in the network that pass through node i\n",
    "- Katz centrality - Generalization of Degree centrality\n",
    "- PageRank centrality - I am more important if more important nodes are connected to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "degree = nx.degree(G)\n",
    "closeness = nx.closeness_centrality(G)\n",
    "pagerank = nx.pagerank(G)\n",
    "betweeness = nx.betweenness_centrality(G)\n",
    "katz = nx.katz_centrality(G)\n",
    "\n",
    "nx.set_node_attributes(G, degree, 'degree')\n",
    "nx.set_node_attributes(G, closeness, 'closeness')\n",
    "nx.set_node_attributes(G, pagerank, 'pagerank')\n",
    "nx.set_node_attributes(G, betweeness, 'betweeness')\n",
    "nx.set_node_attributes(G, katz, 'katz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Community - lauvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "nx.set_node_attributes(G, partition, 'louvain_community')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of df['column'] in linear and log\n",
    "unique_val = np.unique(df['column'], return_counts=True)\n",
    "deg_df = pd.DataFrame({'degree': unique_val[0], 'count': unique_val[1]})\n",
    "deg_df.sort_values(by='degree', inplace=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(deg_df.degree, deg_df['count'], '.-', label='linear')\n",
    "axes[1].loglog(deg_df.degree, deg_df['count'], '.-', label='log')\n",
    "fig.suptitle('In-degree distribution')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
