{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d0380f",
   "metadata": {},
   "source": [
    "# Homework 2 (HW2)\n",
    "By the end of this homework, we expect you to be able to:\n",
    "\n",
    "- Preprocess data and make it amenable to statistical analysis and machine learning models;\n",
    "- Train and test out-of-the-box machine learning models in Python;\n",
    "- Carry out simple multivariate regression analyses;\n",
    "- Use techniques to control for covariates;\n",
    "- Conduct an observational study and reason about its results.\n",
    "\n",
    "---\n",
    "\n",
    "- Homework release: Fri 17 Nov 2023\t\n",
    "\n",
    "- **Homework Due**: Fri 01 Dec 2023, 23:59\t\n",
    "\n",
    "- Grades released: Mon 11 Dec 2023\t\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Some rules\n",
    "1. You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you may do so, but must justify your choice.\n",
    "\n",
    "2. Make sure you use the `data` folder provided in the repository in read-only mode. (Or alternatively, be sure you don’t change any of the files.)\n",
    "\n",
    "3. Be sure to provide a concise textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice.\n",
    "\n",
    "4. For questions containing the **/Discuss:/** prefix, answer not with code, but with a textual explanation **(in markdown)**.\n",
    "\n",
    "5. Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "\n",
    "6. Please write all your comments in **English**, and use meaningful variable names in your code. Your repo should have a single notebook (plus the required data files) in the master/main branch. **If there are multiple notebooks present, we will not grade anything.**\n",
    "\n",
    "7. We will **not run your notebook for you!** Rather, we will grade it as is, which means that only the results contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. Thus, be sure to hand in a **fully-run and evaluated notebook**. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "\n",
    "8. In continuation to the previous point, interactive plots, such as those generated using the `plotly` package, should be strictly avoided!\n",
    "\n",
    "9. Make sure to print results and/or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68937ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T17:54:27.233757Z",
     "start_time": "2023-11-17T17:54:26.757026Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# regression / matching\n",
    "import statsmodels.formula.api as smf\n",
    "import networkx as nx\n",
    "\n",
    "# machine lerning\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# statistics\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04befc",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "After two years, the EPFL Baseball Club is broke. The new Dean transferred all funds to EPFL's new poster child: its super-competitive Pétanque club. After struggling so much to learn about baseball, you have unfortunately been laid off...\n",
    "\n",
    "*(...) 1 month after, you manage to get another job (!) (...)*\n",
    "\n",
    "Congratulations! You have just been hired as a data scientist at the Association for Computational Linguistics (ACL), a professional organization for people working on natural language processing. The ACL organizes several of the top conferences and workshops in the field of computational linguistics and natural language processing.\n",
    "Your boss, Dr. Tiancheng, knows of your expertise in observational studies and asks you to investigate a question that’s been bothering everyone who has ever submitted a paper to a conference: should I spend time on writing rebuttals?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Rebuttals, conferences, and getting your papers accepted\n",
    "\n",
    "Rebuttals in ACL (Association for Computational Linguistics) conferences and in many other academic conferences are an important part of the peer-review process. They allow authors of submitted papers to respond to the reviews and comments provided by the reviewers before a final decision is made regarding the acceptance of the paper. Here's how the rebuttal process typically works in ACL conferences:\n",
    "\n",
    "- Paper Submission: Authors submit their research papers to the ACL conference for review. These papers present novel research findings in computational linguistics, natural language processing, and related areas.\n",
    "- Peer Review: The papers undergo a peer-review process after the initial submission. The program committee or reviewers are experts in the field who evaluate the papers based on their quality, significance, novelty, methodology, and other relevant criteria. They provide comments and scores for each paper.\n",
    "- Rebuttal Period: After receiving the reviews, authors are given a specific period (usually around a week) to write a rebuttal. The rebuttal is a formal response to the reviewers' comments. It allows authors to clarify misunderstandings, address concerns, and provide additional information to support their paper's quality. \n",
    "- Final Review: After receiving the rebuttals, the reviewers may reconsider their initial assessments in light of the authors' responses. Reviewers may choose to maintain or adjust their reviews and scores based on the quality and effectiveness of the author's rebuttal.\n",
    "- Final Decision: The program committee or conference organizers consider the initial reviews/scores, rebuttals, and revised reviews/scores to make a final decision on the acceptance of the papers. The decision can be acceptance, rejection, or conditional acceptance with a request for revisions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "- `tmp_id`: Unique identifier for each paper in the format \"P{number}\".\n",
    "- `status`: Accept or Reject.\n",
    "- `submission_type`: Short vs. Long (papers can have different lengths). We do not use this column in this homework. \n",
    "- `track`: Track to which the paper was submitted, broadly speaking, the \"topic\" of the paper.\n",
    "- `scores_before`: Scores received before rebuttal. This is a nested JSON with many fields, but we will use only the \"overall\" score for the homework. \n",
    "- `scores_after`: Scores received after rebuttal. This is a nested JSON with many fields, but we will use only the \"overall\" score for the homework.\n",
    "- `had_rebuttal`: True or False.\n",
    "\n",
    "\n",
    "Note that: \n",
    " - reviews are assigned numbers, e.g., \"2\";\n",
    " - papers can have different numbers of reviews;\n",
    " - review numbers are arbitrary, e.g., `P1` in the dataframe has two reviews numbered \"2\" and \"3\" (but no review \"1\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28074a9c",
   "metadata": {},
   "source": [
    "## Task 1 (10 pts): Get to Know Your Data\n",
    "\n",
    "As a good data scientist, you first load the data and perform some small sanity checks on it.\n",
    "\n",
    "- You are expected to continuously alter your dataframe as you complete the tasks. E.g., if you are asked to filter the data in a specific task, continue using the filtered dataset in the subsequent tasks.\n",
    "- When we tell you to \"print the dataframe,\" make sure you print it in a way that shows the total number of rows and columns in it (`display(df)` should suffice)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc66e675-eb6b-4ccb-a46c-02ed2e3b4a4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### In all the plots we are using colorblind-friendly colors from [this](https://davidmathlogic.com/colorblind/#%23FFC20A-%230C7BDC) website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ab063",
   "metadata": {},
   "source": [
    "**1.1** Load the dataset containing ACL reviews into memory using pandas. \n",
    "- For each paper, create columns `overall_score_before_avg` and `overall_score_after_avg` containing the average (overall) scores before and after rebuttal.\n",
    "- For each paper, create columns `overall_score_before_std` and `overall_score_after_std` containing the standard deviation of the overall scores before and after the rebuttal.\n",
    "- Print the four newly created columns for paper `P17`.\n",
    "- Print the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f773655-0827-4cd8-9b8e-d634ded1b174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reviews = pd.read_json(\"./data/acl18_v1_numerical_final.json\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549babfa-8a83-45d2-9f16-88d88dc02776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_score_before_avg</th>\n",
       "      <th>overall_score_after_avg</th>\n",
       "      <th>overall_score_before_std</th>\n",
       "      <th>overall_score_after_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    overall_score_before_avg  overall_score_after_avg  \\\n",
       "16                       4.5                      4.5   \n",
       "\n",
       "    overall_score_before_std  overall_score_after_std  \n",
       "16                       0.5                      0.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# helper function to use later on as argument to the pandas 'apply' fucntion\n",
    "def get_overall_score(x, col, mean):\n",
    "    '''\n",
    "    x: one row/paper of the data frame \n",
    "    col: column/feature we want to use from the data frame  \n",
    "    mean: boolean (1 or 0) if set to 1 the function will return mean, else it will return std \n",
    "    returns: the average_overall_score for this row/paper\n",
    "    '''\n",
    "    x_dict = dict(x[col])\n",
    "    overall_scores = []\n",
    "    # iterate on the reviews and get the given overall score\n",
    "    for k in x_dict.keys():\n",
    "        overall_scores.append(x_dict[k][\"scores\"][\"overall_score\"])\n",
    "        \n",
    "    return np.mean(overall_scores) if mean else np.std(overall_scores)\n",
    "\n",
    "# create the required columns \n",
    "df_reviews[\"overall_score_before_avg\"] = df_reviews.apply(\n",
    "    lambda x: get_overall_score(x, col = \"scores_before\", mean=1), axis=1\n",
    ")\n",
    "df_reviews[\"overall_score_after_avg\"] = df_reviews.apply(\n",
    "    lambda x: get_overall_score(x, col = \"scores_after\", mean=1), axis=1\n",
    ")\n",
    "df_reviews[\"overall_score_before_std\"] = df_reviews.apply(\n",
    "    lambda x: get_overall_score(x, col = \"scores_before\", mean=0), axis=1\n",
    ")\n",
    "df_reviews[\"overall_score_after_std\"] = df_reviews.apply(\n",
    "    lambda x: get_overall_score(x, col = \"scores_after\", mean=0), axis=1\n",
    ")\n",
    "\n",
    "# print the four newly created columns for paper P17\n",
    "display(df_reviews[df_reviews[\"tmp_id\"] == \"P17\"][['overall_score_before_avg', 'overall_score_after_avg', \n",
    "                                           'overall_score_before_std', 'overall_score_after_std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c131535-c7da-4176-93bb-b987da5a945d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmp_id</th>\n",
       "      <th>status</th>\n",
       "      <th>submission_type</th>\n",
       "      <th>track</th>\n",
       "      <th>scores_before</th>\n",
       "      <th>scores_after</th>\n",
       "      <th>had_rebuttal</th>\n",
       "      <th>overall_score_before_avg</th>\n",
       "      <th>overall_score_after_avg</th>\n",
       "      <th>overall_score_before_std</th>\n",
       "      <th>overall_score_after_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>{'2': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'2': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.942809</td>\n",
       "      <td>0.942809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Short</td>\n",
       "      <td>Multidisciplinary and Area Chair COI</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P4</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Short</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>1.247219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P5</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Document Analysis</td>\n",
       "      <td>{'2': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'2': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>P1541</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Short</td>\n",
       "      <td>Textual Inference and Other Areas of Semantics</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0.471405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>P1542</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>P1543</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Machine Learning</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.942809</td>\n",
       "      <td>0.942809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>P1544</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Short</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>P1545</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Short</td>\n",
       "      <td>Information Extraction and Text Mining</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1538 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tmp_id  status submission_type  \\\n",
       "0        P1  Reject            Long   \n",
       "1        P2  Reject            Long   \n",
       "2        P3  Accept           Short   \n",
       "3        P4  Reject           Short   \n",
       "4        P5  Reject            Long   \n",
       "...     ...     ...             ...   \n",
       "1533  P1541  Reject           Short   \n",
       "1534  P1542  Reject            Long   \n",
       "1535  P1543  Reject            Long   \n",
       "1536  P1544  Reject           Short   \n",
       "1537  P1545  Reject           Short   \n",
       "\n",
       "                                               track  \\\n",
       "0                                   Machine Learning   \n",
       "1                                 Question Answering   \n",
       "2               Multidisciplinary and Area Chair COI   \n",
       "3                                   Machine Learning   \n",
       "4                                  Document Analysis   \n",
       "...                                              ...   \n",
       "1533  Textual Inference and Other Areas of Semantics   \n",
       "1534                                Machine Learning   \n",
       "1535                                Machine Learning   \n",
       "1536                                    Social Media   \n",
       "1537          Information Extraction and Text Mining   \n",
       "\n",
       "                                          scores_before  \\\n",
       "0     {'2': {'scores': {'originality': 2, 'soundness...   \n",
       "1     {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "2     {'1': {'scores': {'originality': 4, 'soundness...   \n",
       "3     {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "4     {'2': {'scores': {'originality': 2, 'soundness...   \n",
       "...                                                 ...   \n",
       "1533  {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "1534  {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "1535  {'1': {'scores': {'originality': 3, 'soundness...   \n",
       "1536  {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "1537  {'1': {'scores': {'originality': 3, 'soundness...   \n",
       "\n",
       "                                           scores_after  had_rebuttal  \\\n",
       "0     {'2': {'scores': {'originality': 2, 'soundness...          True   \n",
       "1     {'1': {'scores': {'originality': 2, 'soundness...          True   \n",
       "2     {'1': {'scores': {'originality': 4, 'soundness...          True   \n",
       "3     {'1': {'scores': {'originality': 2, 'soundness...          True   \n",
       "4     {'2': {'scores': {'originality': 2, 'soundness...          True   \n",
       "...                                                 ...           ...   \n",
       "1533  {'1': {'scores': {'originality': 2, 'soundness...          True   \n",
       "1534  {'1': {'scores': {'originality': 2, 'soundness...         False   \n",
       "1535  {'1': {'scores': {'originality': 3, 'soundness...          True   \n",
       "1536  {'1': {'scores': {'originality': 2, 'soundness...         False   \n",
       "1537  {'1': {'scores': {'originality': 3, 'soundness...          True   \n",
       "\n",
       "      overall_score_before_avg  overall_score_after_avg  \\\n",
       "0                     2.500000                 2.500000   \n",
       "1                     3.333333                 3.333333   \n",
       "2                     4.666667                 4.666667   \n",
       "3                     3.000000                 2.666667   \n",
       "4                     3.000000                 2.500000   \n",
       "...                        ...                      ...   \n",
       "1533                  2.333333                 2.333333   \n",
       "1534                  2.000000                 2.000000   \n",
       "1535                  2.666667                 2.666667   \n",
       "1536                  2.000000                 2.000000   \n",
       "1537                  3.000000                 3.000000   \n",
       "\n",
       "      overall_score_before_std  overall_score_after_std  \n",
       "0                     0.500000                 0.500000  \n",
       "1                     0.942809                 0.942809  \n",
       "2                     0.471405                 0.471405  \n",
       "3                     0.816497                 1.247219  \n",
       "4                     0.000000                 0.500000  \n",
       "...                        ...                      ...  \n",
       "1533                  0.471405                 0.471405  \n",
       "1534                  0.816497                 0.816497  \n",
       "1535                  0.942809                 0.942809  \n",
       "1536                  0.000000                 0.000000  \n",
       "1537                  0.816497                 0.816497  \n",
       "\n",
       "[1538 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the resulting df\n",
    "display(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ebff",
   "metadata": {},
   "source": [
    "**1.2** Create a single plot with 14 inches of width and 4 inches of height. The plot should contain two panels: \n",
    "- **Panel A**: The distribution of `overall_score_before_avg` for papers that were accepted and papers that were rejected.\n",
    "- **Panel B**: The distribution of `overall_score_before_avg` for papers that had rebuttals vs. papers that did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd34712-b290-45bd-8c7b-ccf04ecc0f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14,4), sharex=True, sharey=True)\n",
    "fig.suptitle(\"The distribution of overall_score_before_avg\")\n",
    "\n",
    "#plotting the histograms/distributions (not cumulative)\n",
    "axs[0].hist(df_reviews[df_reviews[\"status\"] == \"Accept\"].overall_score_before_avg, label=\"Accepted\", \n",
    "            histtype='step', color='#0C7BDC', density=True)\n",
    "axs[0].hist(df_reviews[df_reviews[\"status\"] == \"Reject\"].overall_score_before_avg, label=\"Rejected\", \n",
    "            histtype='step', color='#FFC20A', density=True)\n",
    "axs[0].set(xlabel=\"overall_score_before_avg\", ylabel=\"Distribution\")\n",
    "axs[0].legend()\n",
    "axs[0].set_title(\"Panel A\")\n",
    "\n",
    "axs[1].hist(df_reviews[df_reviews[\"had_rebuttal\"] == True].overall_score_before_avg, label=\"had rebuttal\", \n",
    "            histtype='step', color='#0C7BDC', density=True)\n",
    "axs[1].hist(df_reviews[df_reviews[\"had_rebuttal\"] == False].overall_score_before_avg, label=\"did not have rebuttal\", \n",
    "            histtype='step', color='#FFC20A', density=True)\n",
    "axs[1].set(xlabel=\"overall_score_before_avg\")\n",
    "axs[1].legend()\n",
    "axs[1].set_title(\"Panel B\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea160c7",
   "metadata": {},
   "source": [
    "**1.3** **/Discuss/:** If you know a paper had a rebuttal, is it more or less likely that it was accepted? Does this mean that rebuttals help papers get accepted? Explain why or why not, providing a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbcd7a-b6c0-4d80-b4e2-2b53fbaeb51c",
   "metadata": {},
   "source": [
    "Knowing that a paper had a rebuttal, the paper's overall score average before will follow the blue curve from panel B. We see that the distribution is slightly left-skewed which means the mode is higher than the mean and so it is more likely for a paper that it got a high score and thus from panel A it's more likely that it was accepted (as the \"accepted\"/blue curve maximizes the likelihood of having a high score).\n",
    "This however, doesn't mean that rebuttals help papers get accepted as confounders may be present; in fact the variable `track` can potentially influence the acceptance rate i.e some tracks may have higher acceptance rates than others and also influence the `had_rebuttal` variable in which case the `track` will be a *confounder* and `had_rebuttal` won't cause the papers to get accepted. Another \"common sense\" reason, is that there's still the **`overall_score_average_after`** that needs to be taken into account. In fact, a paper might get a high score before, then give a rebuttal and get a lower score after and get rejected. This case happened for paper P4. This could happen if for example the rebuttal wasn't of high quality, or if it unveiled a flaw that was previously hidden in the research. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87836f37",
   "metadata": {},
   "source": [
    "**1.4** Print the percentage of rebuttals per track in the conference (defined by the `track` column). \n",
    "\n",
    "**/Discuss:/** Using \"the logic\" of hypothesis testing (see slide 29 of Lecture 4), how would you devise a statistical test to refute the following null hypothesis: all tracks have the same fraction of papers with rebuttals. Your statistical test should consider all categories at once, rather than comparing the fraction of rebuttals between pairs of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190be4e7-9538-4d4c-8264-0e15cf4d311d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track\n",
       "Dialogue and Interactive Systems                                0.775281\n",
       "Discourse and Pragmatics                                        0.804348\n",
       "Document Analysis                                               0.730000\n",
       "Generation                                                      0.779661\n",
       "Information Extraction and Text Mining                          0.768362\n",
       "Linguistic Theories Cognitive Modeling and Psycholinguistics    0.750000\n",
       "Machine Learning                                                0.808696\n",
       "Machine Translation                                             0.820755\n",
       "Multidisciplinary and Area Chair COI                            0.680000\n",
       "Multilinguality                                                 0.806452\n",
       "Phonology Morphology and Word Segmentation                      0.851852\n",
       "Question Answering                                              0.728395\n",
       "Resources and Evaluation                                        0.732394\n",
       "Sentence-level semantics                                        0.788889\n",
       "Sentiment Analysis and Argument Mining                          0.788235\n",
       "Social Media                                                    0.737705\n",
       "Summarization                                                   0.745098\n",
       "Tagging Chunking Syntax and Parsing                             0.770492\n",
       "Textual Inference and Other Areas of Semantics                  0.771930\n",
       "Vision Robotics Multimodal Grounding and Speech                 0.811321\n",
       "Word-level Semantics                                            0.860759\n",
       "Name: had_rebuttal, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mean of the 'had_rebuttal' variable is the % of rebuttals\n",
    "rebuttal_perc = df_reviews.groupby(\"track\").had_rebuttal.mean()\n",
    "#print the rebutalls' percentages per track\n",
    "display(rebuttal_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8b3c4-56ec-467a-aacc-5b8f5fe43565",
   "metadata": {},
   "source": [
    "We can first assume that the null hypothesis is true i.e. that all tracks have the same fraction of papers with rebuttals. Then, we draw a barplot showing the fraction of papers with rebuttals for each track along with error bars for a 95% confidence interval. If at least 2 intervals are **non-overlapping** (i.e p-value < 0.05 for a 95% ci) then the null hypothesis doesn't explain what we observe and we can reject it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50275647",
   "metadata": {},
   "source": [
    "## Task 2 (10pts): Prediction\n",
    "\n",
    "You decide to investigate further the effect of rebuttals on acceptance using your machine learning skills."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578baafa",
   "metadata": {},
   "source": [
    "\n",
    "**2.1** For each possible value in the `track` column, create a new column called {track}-onehot (e.g., for track=Generation, create Generation-onehot). Collectively, these new columns should \"one hot-encode\" the track column---for instance, if for a given paper the `track` column is filled with the value \"Generation\", the Generation-onehot column should equal 1 and all other {track}-onehot columns should equal 0. \n",
    "\n",
    "Print the column names of the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0483b9a2-779b-4fdc-8f6a-cefad391ce34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tmp_id', 'status', 'submission_type', 'track', 'scores_before',\n",
      "       'scores_after', 'had_rebuttal', 'overall_score_before_avg',\n",
      "       'overall_score_after_avg', 'overall_score_before_std',\n",
      "       'overall_score_after_std', 'Dialogue and Interactive Systems-onehot',\n",
      "       'Discourse and Pragmatics-onehot', 'Document Analysis-onehot',\n",
      "       'Generation-onehot', 'Information Extraction and Text Mining-onehot',\n",
      "       'Linguistic Theories Cognitive Modeling and Psycholinguistics-onehot',\n",
      "       'Machine Learning-onehot', 'Machine Translation-onehot',\n",
      "       'Multidisciplinary and Area Chair COI-onehot', 'Multilinguality-onehot',\n",
      "       'Phonology Morphology and Word Segmentation-onehot',\n",
      "       'Question Answering-onehot', 'Resources and Evaluation-onehot',\n",
      "       'Sentence-level semantics-onehot',\n",
      "       'Sentiment Analysis and Argument Mining-onehot', 'Social Media-onehot',\n",
      "       'Summarization-onehot', 'Tagging Chunking Syntax and Parsing-onehot',\n",
      "       'Textual Inference and Other Areas of Semantics-onehot',\n",
      "       'Vision Robotics Multimodal Grounding and Speech-onehot',\n",
      "       'Word-level Semantics-onehot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#create the one hot encoder renaming the created columns to what is required i.e {track}-onehot\n",
    "custom_enc = OneHotEncoder(feature_name_combiner=(lambda x,y: y+'-onehot'))\n",
    "#fit the encoder to the 'track' column and transform the column  \n",
    "ohe_track = custom_enc.fit_transform(np.array(df_reviews['track']).reshape(-1,1)).toarray()\n",
    "#create a pandas df from the one hot encoded representation of the 'track' column\n",
    "ohe_track_df = pd.DataFrame(ohe_track, columns=custom_enc.get_feature_names_out(['track']))\n",
    "#update df (i.e remove the track column and replace it with the newly obtained binary features)\n",
    "df_reviews = pd.concat([df_reviews, ohe_track_df], axis=1)\n",
    "\n",
    "print(df_reviews.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa41ea",
   "metadata": {},
   "source": [
    "\n",
    "**2.2** Create a column `had_rebuttal_int`, which equals 1 if the paper had a rebuttal, and 0 otherwise, and a column `accepted_int`, which equals 1 if the paper was accepted, and 0 otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c6b687-a7a9-4d85-be45-d738a01d78a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reviews['had_rebuttal_int'] = (df_reviews['had_rebuttal']).astype(int) \n",
    "df_reviews['accepted_int'] = (df_reviews['status'] == 'Accept').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5806400",
   "metadata": {},
   "source": [
    "**2.3** Create a function `numpy_helper(df, cols)` to obtain a numpy.array out of your dataframe. The function should receive a dataframe `df` with N rows and a list of M columns `cols`, and should return a `np.array` of dimension `(NxM)` cast as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75098907-1364-4120-bc1b-b0f896a1bf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def numpy_helper(df, cols):\n",
    "    '''\n",
    "    df: a dataframe with N rows\n",
    "    cols: a list of M columns\n",
    "    \n",
    "    return: np.array of dimension (NxM) cast as a float.\n",
    "    '''\n",
    "    return np.array(df[cols]).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dad455",
   "metadata": {},
   "source": [
    "\n",
    "**2.4**\n",
    "Create:\n",
    "- an array of features X containing all track one-hot features, as well as the `overall_score_before_avg`,`overall_score_before_std`, and `had_rebuttal_int`;\n",
    "- an array of outcomes y containing `accepted_int`. \n",
    "\n",
    "\n",
    "Print the shapes of both X and y (e.g., `X.shape`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e3cb506-de1a-4be0-b7f3-cee8eca03875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of array X is: (1538, 24)\n",
      "The shape of array y is: (1538, 1)\n"
     ]
    }
   ],
   "source": [
    "#creating the X array using the helper function from previous question\n",
    "cols_X = list(custom_enc.get_feature_names_out(['track'])) + ['overall_score_before_avg', \n",
    "                                                              'overall_score_before_std', 'had_rebuttal_int']\n",
    "X = numpy_helper(df_reviews, cols_X)\n",
    "\n",
    "#creating the y array using the helper function from previous question\n",
    "cols_y = ['accepted_int']\n",
    "y = numpy_helper(df_reviews, cols_y)\n",
    "\n",
    "#printing the shapes\n",
    "print(\"The shape of array X is:\", X.shape)\n",
    "print(\"The shape of array y is:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf79a4",
   "metadata": {},
   "source": [
    "\n",
    "**2.5** Build two `GradientBoostingClassifier` models using `sklearn` using the default parameters:\n",
    "- Model 1: predicts the outcome `accepted_int` using the onehot encoded features related to track, as well as the `overall_score_before_avg`,`overall_score_before_std`.\n",
    "- Model 2:  predicts the outcome `accepted_int` using the onehot encoded features related to track, as well as the `overall_score_before_avg`,`overall_score_before_std` **and** `had_rebuttal_int`.\n",
    "\n",
    "\n",
    "For both models:\n",
    "\n",
    "- Use the `cross_validate` function from `sklearn.model_selection` to compute the average precision, recall, and accuracy across test cross validation splits.\n",
    "\n",
    "    - e.g., `cross_validate(clf, X, y, cv=30, scoring=('accuracy', 'precision', 'recall'))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9978f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Accuracy: 0.84 (+/- 0.08)\n",
      "Precision: 0.71 (+/- 0.23)\n",
      "Recall: 0.59 (+/- 0.23)\n",
      "\n",
      "Model 2\n",
      "Accuracy: 0.84 (+/- 0.07)\n",
      "Precision: 0.71 (+/- 0.22)\n",
      "Recall: 0.60 (+/- 0.23)\n"
     ]
    }
   ],
   "source": [
    "scores_1 = cross_validate(GradientBoostingClassifier(), X[:,:-1], y.ravel(), cv=30, \n",
    "                                                          scoring=['accuracy', 'precision', 'recall'])\n",
    "scores_2 = cross_validate(GradientBoostingClassifier(), X, y.ravel(), cv=30, \n",
    "                                                          scoring=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "# print the average accuracy, precision, and recall across test cross validation splits for model 1\n",
    "print(\"Model 1\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_1['test_accuracy'].mean(), scores_1['test_accuracy'].std() * 2))\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores_1['test_precision'].mean(), scores_1['test_precision'].std() * 2))\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (scores_1['test_recall'].mean(), scores_1['test_recall'].std() * 2))\n",
    "\n",
    "# print the average accuracy, precision, and recall across test cross validation splits for model 2\n",
    "print(\"\\nModel 2\")\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores_2['test_accuracy'].mean(), scores_2['test_accuracy'].std() * 2))\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores_2['test_precision'].mean(), scores_2['test_precision'].std() * 2))\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (scores_2['test_recall'].mean(), scores_2['test_recall'].std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abba6d4",
   "metadata": {},
   "source": [
    "\n",
    "**2.6** Determine whether the difference in accuracy of the two models is statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f907fad9-20bc-4776-bd3b-b0e685df5815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.1291661307585383, pvalue=0.897673080747567)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running a t-test to compare the accuracy means of models 1 and 2\n",
    "ttest_ind(scores_1['test_accuracy'], scores_2['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a76579-0e6b-46a3-8b81-57269f032279",
   "metadata": {},
   "source": [
    "We can see that the p-value is ~ 0.89 > 0.05 which means that the null hypothesis (the 2 mean accuracies are equal) is enough to explain what we observe. Thus, the difference in accurancy of the 2 models is NOT statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518f2f6",
   "metadata": {},
   "source": [
    "**2.7** **/Discuss:/** Contrast the results obtained in **2.6** with what you observed in **Task 1**. What advantage did the analyses in **2.6** have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28149e0c-2f09-4ce2-9d0e-a4896be42c89",
   "metadata": {},
   "source": [
    "In 2.6 we found out that the had_rebuttal variable has no statistically significant influence on the acceptance of a paper which contradicts what we said in task 1 (we said that when a paper had a rebuttal then it is *more likely* to get accepted (note that we didn't say it causes a paper acceptance, but only increases its probability)). \n",
    "The advantage the analyses in 2.6 have is that they split the categories of the track feature into separate one hot encoded features while in task 1 we don't. This is better as we've seen that not all tracks have the same rebuttal percentage for papers and thus should be considered separately in our prediction of papers' acceptance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52beb97e",
   "metadata": {},
   "source": [
    "## Task 3 (12pts): Interlude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f56eca",
   "metadata": {},
   "source": [
    "\n",
    "**3.1** Using the formula API from `statsmodels`, estimate the following linear regressions. Report the summary of the models.\n",
    "- `accepted_int ~ had_rebuttal_int`,  \n",
    "- `accepted_int ~ overall_score_after_avg`\n",
    "- `had_rebuttal_int ~ overall_score_before_avg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29288b10-92a6-4faa-bb8f-410b4619bae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform ols regression helper function\n",
    "def ols_regression(df: pd.DataFrame, formula: str) -> None:\n",
    "    \"\"\"\n",
    "    Computes the OLS regression for the given formula and dataframe,\n",
    "    and reports the results.\n",
    "    :param df: the dataframe to use\n",
    "    :param formula: the formula to use\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # create a fitted model\n",
    "    mod = smf.ols(formula=formula, data=df)\n",
    "    # Fits the model (find the optimal coefficients, adding a random seed ensures consistency)\n",
    "    np.random.seed(2)\n",
    "    res = mod.fit()\n",
    "    # Print the summary output provided by the library.\n",
    "    print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc0028fe-0bc4-42b1-b6ba-6e334ead8828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           accepted_int   R-squared:                       0.041\n",
      "Model:                            OLS   Adj. R-squared:                  0.041\n",
      "Method:                 Least Squares   F-statistic:                     66.22\n",
      "Date:                Fri, 01 Dec 2023   Prob (F-statistic):           8.24e-16\n",
      "Time:                        16:23:25   Log-Likelihood:                -855.16\n",
      "No. Observations:                1538   AIC:                             1714.\n",
      "Df Residuals:                    1536   BIC:                             1725.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept            0.0838      0.023      3.693      0.000       0.039       0.128\n",
      "had_rebuttal_int     0.2098      0.026      8.138      0.000       0.159       0.260\n",
      "==============================================================================\n",
      "Omnibus:                      271.753   Durbin-Watson:                   1.920\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              324.377\n",
      "Skew:                           1.075   Prob(JB):                     3.65e-71\n",
      "Kurtosis:                       2.336   Cond. No.                         4.00\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols_regression(df_reviews, 'accepted_int ~ had_rebuttal_int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d12f03-4449-4594-9291-78f8711fc39e",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Answer:**\n",
    "`accepted_int` = 0.0838 + 0.2098 * `had_rebuttal_int`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a8520aa-f590-473b-9fce-32ad3abe87f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           accepted_int   R-squared:                       0.402\n",
      "Model:                            OLS   Adj. R-squared:                  0.401\n",
      "Method:                 Least Squares   F-statistic:                     1031.\n",
      "Date:                Fri, 01 Dec 2023   Prob (F-statistic):          1.58e-173\n",
      "Time:                        16:23:25   Log-Likelihood:                -492.65\n",
      "No. Observations:                1538   AIC:                             989.3\n",
      "Df Residuals:                    1536   BIC:                             1000.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                  -0.6558      0.029    -22.339      0.000      -0.713      -0.598\n",
      "overall_score_after_avg     0.2860      0.009     32.111      0.000       0.269       0.303\n",
      "==============================================================================\n",
      "Omnibus:                      110.778   Durbin-Watson:                   1.927\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               51.680\n",
      "Skew:                           0.256   Prob(JB):                     5.99e-12\n",
      "Kurtosis:                       2.263   Cond. No.                         12.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols_regression(df_reviews, 'accepted_int ~ overall_score_after_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161d3b4-dc61-46b0-a108-53901a302061",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "`accepted_int` = 0.1651 + 0.2860 * `overall_score_after_avg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe8b3b41-6531-4cf1-a725-7843dc4fab2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       had_rebuttal_int   R-squared:                       0.135\n",
      "Model:                            OLS   Adj. R-squared:                  0.135\n",
      "Method:                 Least Squares   F-statistic:                     240.2\n",
      "Date:                Fri, 01 Dec 2023   Prob (F-statistic):           1.89e-50\n",
      "Time:                        16:23:25   Log-Likelihood:                -727.42\n",
      "No. Observations:                1538   AIC:                             1459.\n",
      "Df Residuals:                    1536   BIC:                             1470.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "============================================================================================\n",
      "                               coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "Intercept                    0.2527      0.035      7.195      0.000       0.184       0.322\n",
      "overall_score_before_avg     0.1651      0.011     15.499      0.000       0.144       0.186\n",
      "==============================================================================\n",
      "Omnibus:                      201.621   Durbin-Watson:                   1.930\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              288.526\n",
      "Skew:                          -1.060   Prob(JB):                     2.23e-63\n",
      "Kurtosis:                       2.890   Cond. No.                         12.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols_regression(df_reviews, 'had_rebuttal_int ~ overall_score_before_avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964faa4-3cb0-4695-9413-1966b9976168",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "`had_rebuttal_int` = 0.2527 + 0.1651 * `overall_score_before_avg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bc5e23",
   "metadata": {},
   "source": [
    "\n",
    "**3.2** **/Discuss:/** Interpret the coefficients associated with the binary independent variable in the above models. Note that independent variables are the ones on the right-handside of the equation.\n",
    "\n",
    "- e.g., in `had_rebuttal_int ~ overall_score_before_avg`, `overall_score_before_avg` is the independent variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e0d8ae-3efd-4d88-b942-6a219c3d5b77",
   "metadata": {},
   "source": [
    "The coefficient asscociated with the binary independent variable `had_rebuttal_int` (i.e the slope) in the formula `accepted_int ~ had_rebuttal_int` is the difference in mean outcomes (i.e mean acceptance rate) between data points with `had_rebuttal_int` = 1 and data points with `had_rebuttal_int` = 0. For instance, papers with `had_rebuttal_int` = 0 have a mean `accepted_int` of 0.0838 (the intercept) (almost 0 acceptance!) while papers with `had_rebuttal_int` = 1 have a mean `accepted_int` of 0.0838 (the intercept) + 0.2098 (the slope) = 0.2936 (~30% acceptance rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba81dc",
   "metadata": {},
   "source": [
    "\n",
    "**3.3** **/Discuss:/** describe three correlations you can draw from the previous analysis. Describe their sign (i.e., whether they are positive or negative), and whether they are statistically significant (at the .05 level of significance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83b55c-0fc5-4f78-8dfd-8d0acab06902",
   "metadata": {},
   "source": [
    "We can see that `accepted_int` is positively correlated with `had_rebuttal_int` as `had_rebuttal_int`'s coefficient (0.2098) is positive and its p-value is 0 < 0.05 so it's statistically significant. This means that a higher value of `had_rebuttal_int` corresponds to a higher value of `accepted_int`.\n",
    "Similarly, we see that `accepted_int` is positively correlated with `overall_score_after_avg` as `overall_score_after_avg`'s coefficient (0.2860) is positive and its p-value is 0 < 0.05 so it's statistically significant. This means that a higher value of `overall_score_after_avg` corresponds to a higher value of `accepted_int`.\n",
    "Likewise, we see that `had_rebuttal_int` is positively correlated with `overall_score_before_avg` as `overall_score_before_avg`'s coefficient (0.1651) is positive and its p-value is 0 < 0.05 so it's statistically significant. This means that a higher value of `overall_score_before_avg` corresponds to a higher value of `had_rebuttal_int`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa59821c",
   "metadata": {},
   "source": [
    "**3.4** **/Discuss:/** Is the following statement True or False? Justify. \n",
    "\n",
    "- The variable `overall_score_after_avg` explains more of the variance in `accepted_int`than the variable `overall_score_before_avg` explains of `had_rebuttal_int`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52a8a3-39c3-4319-a5bd-b929bfffe8e0",
   "metadata": {},
   "source": [
    "True. Explanation: $R^2$ represents the fraction of variance explained by the model. We see that `overall_score_after_avg` explains more of the variance in `accepted_int` than the variable `overall_score_before_avg` explains of `had_rebuttal_int` as the $R^2$ of the former model (0.402) is higher than the $R^2$ of the latter model (0.135)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405fb7f",
   "metadata": {},
   "source": [
    "\n",
    "**3.5** **/Discuss:/** Create a causal diagram relating the following variables:\n",
    "- \"Sa\": `overall_score_after_avg`\n",
    "- \"Sb\": `overall_score_before_avg`\n",
    "- \"Re\": `had_rebuttal_int`\n",
    "- \"Ac\": `accepted_int`\n",
    "- \"Tr\": `track`\n",
    "\n",
    "\n",
    "When unsure about whether a causal relationship exists, include it in the diagram. E.g., include the arrow corresponding to the key questions around this homework, i.e., `had_rebuttal_int`->`accepted_int`, even though you are investigating whether it exists. \n",
    "\n",
    "You may draw your diagram using text, use Sa/Sb/Re/Ac/Tr to represent the names of the variables, and simply indicate the causal links, one per line.\n",
    "\n",
    "\n",
    "Instead of drawing something like this:\n",
    "![](./dagv.jpeg)\n",
    "\n",
    "Simply write:\n",
    "\n",
    "- Tr->Sb\n",
    "- Tr->Ac\n",
    "- Tr->Re\n",
    "- Ac->Sb\n",
    "- Re->Sb\n",
    "- Sb->Sa\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44702c33-0c1b-4c9c-ad9b-46f4041d8085",
   "metadata": {},
   "source": [
    "#### **Answer:**\n",
    "- Tr->Sb\n",
    "- Tr->Ac\n",
    "- Tr->Re\n",
    "- Sb->Ac\n",
    "- Sb->Re\n",
    "- Sb->Sa \n",
    "- Sa->Ac\n",
    "- Re->Ac\n",
    "- Re->Sa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe89c00",
   "metadata": {},
   "source": [
    "**3.6** **/Discuss:/** What is the problem of simply comparing the outcomes of papers that had rebuttals with those that did not? Give a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with simply comparing the outcomes of papers that had rebuttals with those that did not is that there might be a confounder that we are not taking into consideration that may be leading us to false\\\n",
    "conclusions about wheter rebuttals influence acceptance or not.\n",
    "For instance, we might think that rebuttals influence acceptance but in fact the track of the paper might be the confounder.\\\n",
    "In fact, some tracks might have a higher acceptance rate than others and also have a higher rebuttal rate than others. In this case, the track is a confounder as it influences both the rebuttal rate and the acceptance rate and thus we can't say that rebuttals influence acceptance.\\\n",
    "To come up with a more robust conclusion about the causal relationship between rebuttals and acceptance, we need to control for confounders.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed68a4",
   "metadata": {},
   "source": [
    "# Task 4 (12 pts): Observational study\n",
    "\n",
    "You decide to use your observational study skills to obtain a concrete answer to the question: do rebuttals increase acceptance?\n",
    "\n",
    " **4.1** Perform exact one-to-one matching considering the `score_before_avg` and the `track` variables. Each paper that had a rebuttal (\"treatment group\") should be matched to a paper that did not have a rebuttal (\"control group\"). \n",
    "- Your matching should be optimal, i.e., the maximum amount of papers possible must be matched. \n",
    "- Print the dataframe of papers in the matched sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50bd5396-d9b2-43f9-8b9a-1c920ba75b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmp_id</th>\n",
       "      <th>status</th>\n",
       "      <th>submission_type</th>\n",
       "      <th>track</th>\n",
       "      <th>scores_before</th>\n",
       "      <th>scores_after</th>\n",
       "      <th>had_rebuttal</th>\n",
       "      <th>overall_score_before_avg</th>\n",
       "      <th>overall_score_after_avg</th>\n",
       "      <th>overall_score_before_std</th>\n",
       "      <th>...</th>\n",
       "      <th>Sentence-level semantics-onehot</th>\n",
       "      <th>Sentiment Analysis and Argument Mining-onehot</th>\n",
       "      <th>Social Media-onehot</th>\n",
       "      <th>Summarization-onehot</th>\n",
       "      <th>Tagging Chunking Syntax and Parsing-onehot</th>\n",
       "      <th>Textual Inference and Other Areas of Semantics-onehot</th>\n",
       "      <th>Vision Robotics Multimodal Grounding and Speech-onehot</th>\n",
       "      <th>Word-level Semantics-onehot</th>\n",
       "      <th>had_rebuttal_int</th>\n",
       "      <th>accepted_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>P1523</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Document Analysis</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>P597</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Short</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>{'3': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>{'3': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>P672</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Information Extraction and Text Mining</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>P353</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Summarization</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>P271</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Phonology Morphology and Word Segmentation</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>P370</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Short</td>\n",
       "      <td>Sentence-level semantics</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>P477</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Sentence-level semantics</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 2, 'soundness...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>P418</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Textual Inference and Other Areas of Semantics</td>\n",
       "      <td>{'2': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>{'2': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>False</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>P593</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 3, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.699673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>P128</td>\n",
       "      <td>Reject</td>\n",
       "      <td>Long</td>\n",
       "      <td>Discourse and Pragmatics</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>{'1': {'scores': {'originality': 4, 'soundness...</td>\n",
       "      <td>True</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>542 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tmp_id  status submission_type  \\\n",
       "1515  P1523  Reject            Long   \n",
       "595    P597  Reject           Short   \n",
       "669    P672  Reject            Long   \n",
       "351    P353  Reject            Long   \n",
       "270    P271  Reject            Long   \n",
       "...     ...     ...             ...   \n",
       "368    P370  Reject           Short   \n",
       "475    P477  Reject            Long   \n",
       "416    P418  Reject            Long   \n",
       "591    P593  Reject            Long   \n",
       "127    P128  Reject            Long   \n",
       "\n",
       "                                               track  \\\n",
       "1515                               Document Analysis   \n",
       "595                              Machine Translation   \n",
       "669           Information Extraction and Text Mining   \n",
       "351                                    Summarization   \n",
       "270       Phonology Morphology and Word Segmentation   \n",
       "...                                              ...   \n",
       "368                         Sentence-level semantics   \n",
       "475                         Sentence-level semantics   \n",
       "416   Textual Inference and Other Areas of Semantics   \n",
       "591                         Resources and Evaluation   \n",
       "127                         Discourse and Pragmatics   \n",
       "\n",
       "                                          scores_before  \\\n",
       "1515  {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "595   {'3': {'scores': {'originality': 3, 'soundness...   \n",
       "669   {'1': {'scores': {'originality': 3, 'soundness...   \n",
       "351   {'1': {'scores': {'originality': 4, 'soundness...   \n",
       "270   {'1': {'scores': {'originality': 4, 'soundness...   \n",
       "...                                                 ...   \n",
       "368   {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "475   {'1': {'scores': {'originality': 2, 'soundness...   \n",
       "416   {'2': {'scores': {'originality': 3, 'soundness...   \n",
       "591   {'1': {'scores': {'originality': 3, 'soundness...   \n",
       "127   {'1': {'scores': {'originality': 4, 'soundness...   \n",
       "\n",
       "                                           scores_after  had_rebuttal  \\\n",
       "1515  {'1': {'scores': {'originality': 2, 'soundness...          True   \n",
       "595   {'3': {'scores': {'originality': 3, 'soundness...          True   \n",
       "669   {'1': {'scores': {'originality': 3, 'soundness...          True   \n",
       "351   {'1': {'scores': {'originality': 4, 'soundness...          True   \n",
       "270   {'1': {'scores': {'originality': 4, 'soundness...          True   \n",
       "...                                                 ...           ...   \n",
       "368   {'1': {'scores': {'originality': 2, 'soundness...          True   \n",
       "475   {'1': {'scores': {'originality': 2, 'soundness...         False   \n",
       "416   {'2': {'scores': {'originality': 3, 'soundness...         False   \n",
       "591   {'1': {'scores': {'originality': 3, 'soundness...          True   \n",
       "127   {'1': {'scores': {'originality': 4, 'soundness...          True   \n",
       "\n",
       "      overall_score_before_avg  overall_score_after_avg  \\\n",
       "1515                  3.000000                 3.000000   \n",
       "595                   2.000000                 2.000000   \n",
       "669                   1.500000                 1.500000   \n",
       "351                   3.333333                 3.333333   \n",
       "270                   2.333333                 2.333333   \n",
       "...                        ...                      ...   \n",
       "368                   2.000000                 2.000000   \n",
       "475                   2.000000                 2.000000   \n",
       "416                   2.500000                 1.500000   \n",
       "591                   2.666667                 2.333333   \n",
       "127                   4.000000                 3.333333   \n",
       "\n",
       "      overall_score_before_std  ...  Sentence-level semantics-onehot  \\\n",
       "1515                  1.000000  ...                              0.0   \n",
       "595                   0.000000  ...                              0.0   \n",
       "669                   0.500000  ...                              0.0   \n",
       "351                   0.471405  ...                              0.0   \n",
       "270                   0.471405  ...                              0.0   \n",
       "...                        ...  ...                              ...   \n",
       "368                   0.000000  ...                              1.0   \n",
       "475                   0.000000  ...                              1.0   \n",
       "416                   1.500000  ...                              0.0   \n",
       "591                   1.699673  ...                              0.0   \n",
       "127                   0.000000  ...                              0.0   \n",
       "\n",
       "      Sentiment Analysis and Argument Mining-onehot  Social Media-onehot  \\\n",
       "1515                                            0.0                  0.0   \n",
       "595                                             0.0                  0.0   \n",
       "669                                             0.0                  0.0   \n",
       "351                                             0.0                  0.0   \n",
       "270                                             0.0                  0.0   \n",
       "...                                             ...                  ...   \n",
       "368                                             0.0                  0.0   \n",
       "475                                             0.0                  0.0   \n",
       "416                                             0.0                  0.0   \n",
       "591                                             0.0                  0.0   \n",
       "127                                             0.0                  0.0   \n",
       "\n",
       "      Summarization-onehot  Tagging Chunking Syntax and Parsing-onehot  \\\n",
       "1515                   0.0                                         0.0   \n",
       "595                    0.0                                         0.0   \n",
       "669                    0.0                                         0.0   \n",
       "351                    1.0                                         0.0   \n",
       "270                    0.0                                         0.0   \n",
       "...                    ...                                         ...   \n",
       "368                    0.0                                         0.0   \n",
       "475                    0.0                                         0.0   \n",
       "416                    0.0                                         0.0   \n",
       "591                    0.0                                         0.0   \n",
       "127                    0.0                                         0.0   \n",
       "\n",
       "      Textual Inference and Other Areas of Semantics-onehot  \\\n",
       "1515                                                0.0       \n",
       "595                                                 0.0       \n",
       "669                                                 0.0       \n",
       "351                                                 0.0       \n",
       "270                                                 0.0       \n",
       "...                                                 ...       \n",
       "368                                                 0.0       \n",
       "475                                                 0.0       \n",
       "416                                                 1.0       \n",
       "591                                                 0.0       \n",
       "127                                                 0.0       \n",
       "\n",
       "      Vision Robotics Multimodal Grounding and Speech-onehot  \\\n",
       "1515                                                0.0        \n",
       "595                                                 0.0        \n",
       "669                                                 0.0        \n",
       "351                                                 0.0        \n",
       "270                                                 0.0        \n",
       "...                                                 ...        \n",
       "368                                                 0.0        \n",
       "475                                                 0.0        \n",
       "416                                                 0.0        \n",
       "591                                                 0.0        \n",
       "127                                                 0.0        \n",
       "\n",
       "      Word-level Semantics-onehot  had_rebuttal_int  accepted_int  \n",
       "1515                          0.0                 1             0  \n",
       "595                           0.0                 1             0  \n",
       "669                           0.0                 1             0  \n",
       "351                           0.0                 1             0  \n",
       "270                           0.0                 1             0  \n",
       "...                           ...               ...           ...  \n",
       "368                           0.0                 1             0  \n",
       "475                           0.0                 0             0  \n",
       "416                           0.0                 0             0  \n",
       "591                           0.0                 1             0  \n",
       "127                           0.0                 1             0  \n",
       "\n",
       "[542 rows x 34 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exact matching when the treatment and control are exact same on average score and track\n",
    "# Select the treatment/control groups \n",
    "treatment_df = df_reviews[df_reviews['had_rebuttal_int']==1]\n",
    "control_df = df_reviews[df_reviews['had_rebuttal_int']==0]\n",
    "\n",
    "# Create Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add an edge with a weight of 1 when the control and treatment groups have the exact same average score and track\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "        if (control_row['track'] == treatment_row['track']) & \\\n",
    "        (control_row['overall_score_before_avg'] == treatment_row['overall_score_before_avg']):\n",
    "            G.add_weighted_edges_from([(control_id, treatment_id, 1)])\n",
    "\n",
    "# Get at most one matched node for each node, maximizing the total weight of matched edges\n",
    "matching = nx.max_weight_matching(G)\n",
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "\n",
    "# Select the matched pairs only\n",
    "df_matched = df_reviews.loc[matched]\n",
    "display(df_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c97d1",
   "metadata": {},
   "source": [
    "**4.2** So far, we did not consider the `score_before_std` variable. One could argue that the variance in the scores makes a difference. E.g., a paper that received scores 1 and 5, might be very different from a paper with scores 3 and 3. \n",
    "\n",
    "Note that you did not match on the `score_before_std` variable. However, it suffices if this variable is \"balanced\" across treatment and control groups.\n",
    " Use the Standardized Mean Difference (SMD) to assess whether that's the case.\n",
    "\n",
    "- The standardized mean difference for a variable $x$ and two groups $t$ and $c$ is defined as: $\\frac{| E[x_t] - E[x_c] |}{\\sqrt{Var[x_t] + Var[x_c]}}$\n",
    "\n",
    "- Note that a Standardized Mean Difference smaller than 0.1 suggests that variables are balanced across treatment and control groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c16b2276-ba5f-47d0-b830-ac33a3d3b1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMD: 0.07632\n"
     ]
    }
   ],
   "source": [
    "# select the treatment/control groups of the matched dataset\n",
    "treatment_df_match = df_matched[df_matched['had_rebuttal_int']==1]\n",
    "control_df_match = df_matched[df_matched['had_rebuttal_int']==0]\n",
    "\n",
    "# compute smd\n",
    "smd = np.abs(np.mean(treatment_df_match['overall_score_before_std']) - \\\n",
    "             np.mean(control_df_match['overall_score_before_std'])) / \\\n",
    "      np.sqrt(np.var(treatment_df_match['overall_score_before_std']) + \\\n",
    "              np.var(control_df_match['overall_score_before_std']))\n",
    "print('SMD: {0:.5}'.format(smd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a0a33-5090-4293-b1b1-d6eb027d58d0",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "We see that smd=0.07632 < 0.1, which suggests that the `overall_score_before_std` is balanced across the 2 groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f788c",
   "metadata": {},
   "source": [
    "\n",
    "**4.3** Using the matched sample, estimate the following linear regression: `accepted ~ had_rebuttal_int`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df6df892-e7e2-482b-8c72-1ce2b8f97e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           accepted_int   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.002\n",
      "Method:                 Least Squares   F-statistic:                   0.07696\n",
      "Date:                Fri, 01 Dec 2023   Prob (F-statistic):              0.782\n",
      "Time:                        16:23:58   Log-Likelihood:                -132.72\n",
      "No. Observations:                 542   AIC:                             269.4\n",
      "Df Residuals:                     540   BIC:                             278.0\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "====================================================================================\n",
      "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept            0.1033      0.019      5.492      0.000       0.066       0.140\n",
      "had_rebuttal_int     0.0074      0.027      0.277      0.782      -0.045       0.060\n",
      "==============================================================================\n",
      "Omnibus:                      278.289   Durbin-Watson:                   1.931\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1033.626\n",
      "Skew:                           2.542   Prob(JB):                    3.56e-225\n",
      "Kurtosis:                       7.463   Cond. No.                         2.62\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# estimate the linear regression using ols module\n",
    "ols_regression(df_matched, 'accepted_int ~ had_rebuttal_int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98b8b2c-45a0-40e0-b3f6-c17b2a462b5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Answer:**\n",
    "`accepted_int` = 0.1033 + 0.0074 * `had_rebuttal_int`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8423e",
   "metadata": {},
   "source": [
    "\n",
    "**4.4** **/Discuss:/**\n",
    "\n",
    "i. Considering your results obtained in 4.3, and the causal diagram drawn in Task 3: do rebuttals increase the chance of a paper getting accepted? Why are results different from what you obtained in **Task 1?**\n",
    "\n",
    "ii. Why is there no need to include other covariates (e.g., score before) in the regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ed34c-8b8d-4c1f-a30f-22a8ffb732cf",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "i. Because the coefficient of `had_rebuttal_int` is very small and its corresponding pvalue > 0.05, we can say that rebuttals don't affect the acceptance of the paper.\n",
    "\n",
    "Here the result is different than what we observed in task 1, because in task 1 we studied the correalation between acceptance and rebuttal directly without considering any confounders. However, `overall_score_before_avg` and `track` are influencing both the rebuttals and acceptance and are confounders. So, here we have balanced papers with and without rebuttals, making the `overall_score_before_avg` and `track` on both groups, and found that having rebuttals does not influence the acceptance of the paper.\n",
    "\n",
    "ii. Because we have balanced the dataset on `track`, `overall_score_before_avg` and given that the matched dataset was already balanced on `overall_score_before_std`, they don't affect the `accepted_int` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4867ea1",
   "metadata": {},
   "source": [
    "**4.5** **/Discuss:/** Imagine there is another, unobserved variable \"quality\" which captures the true quality of the paper. Suppose quality (\"Qu\") is connected to the DAG you drew in the following ways:\n",
    "- Qu -> Sa\n",
    "- Qu -> Sb\n",
    "- Qu -> Re\n",
    "- Qu -> Ac \n",
    "\n",
    "Assume that\n",
    "\n",
    "- quality can only increase the chances of rebuttals;\n",
    "- quality and the rebuttal can only increase the chance of a paper being accepted.\n",
    "\n",
    "Does this uncontrolled confounder threaten the validity of your findings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6310fdf-3329-4a81-8666-3286eba7762d",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "- Before in Task 3.5 we had put the correlation Re -> Ac, as we were still studying it, however after matching the control/treatment groups, we saw that there is actually no direct correlation, but it was because of the presense of confounders `overall_score_before_avg` (Sb) and `track` (Tr). Our finding is that **having a rebuttal does not cause the paper to be accepted.**\n",
    "\n",
    "- The fact that we don't have a correlation Sa -> Re, and we have removed the correlation Sb -> Re by matching the control/treatment group on `overall_score_before_avg` means the correlations Qu -> Sa and Qu -> Sb do not affect our study of the correlation between Re and Ac.\n",
    "\n",
    "- Because \"Qu\" is influencing both Re and Ac, it is an additional confounder that we are not considering in our analysis, so by considering the Quality of the paper, maybe we could have found a direct correlation (meaning causation) between `had_rebuttal_int` and `accepted_int`. However, because the increase of the chance of a paper's acceptance is caused by quality and rebuttal, and the increase of the chances of rebuttals is caused by quality, the increase of the chance of a paper's acceptance might be caused by the quality only, and not be influenced by the rebuttals directly. We could study it by matching on `Quality` as well, if that is the case, it would not have invalidated our current findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
